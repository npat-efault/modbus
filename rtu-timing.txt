
MODBUS Over Serial Line, RTU timing issues
==========================================

CAVEAT EMPTOR:

   Package modbus does NOT use silent intervals to detect the
   boundaries of ModBus Over Serial RTU-encoded frames (messages) as
   described in the relevant spec. Instead, it parses the frame data
   on the fly to determine their length. This works very well for RTU
   masters and imposes no limitations. For RTU slaves, though, this
   approach has significant issues, does impose limitations, and
   introduces flakiness that must be carefully
   controlled. Unfortunately, every other solution would mandate the
   implementation of special-purpose O/S asynchronous serial
   device-drivers, something that is well outside the scope of the
   project.

   These limitations do NOT apply to MobBus Over TCP, neither to
   ModBus over Serial with ASCII frame encoding.

Read on if you wish to understand the details...

The "MODBUS over Serial Line" spec [1], in section 2.5.1.1 describes
how, in RTU encoding, frames are delineated and detected using silent
intervals. While this may have been a reasonable choice in the 70s and
the 80s, it causes significant (and, frankly, unnecessary)
complications in modern computing environments. This document explains
why and the consequences of this.

Modern (and not so modern) UARTs have hardware FIFO queues on their
receivers. While these FIFOs can be disabled, and have the UART
operate in "one character at a time" mode, this is not always
practicable (you would have to write your own or modify the O/S UART
device driver) or desirable (it could causes high interrupt rates at
high baudrates, which may be overwhelming, especially if you are using
multiple ports at the same time).

Assume a 16-chars FIFO (a typical size) configured to interrupt when
it's 100% full (the "worst case" for our discussion). Also assume a
bitrate of 9600bps. In this case, when the line is fully driven
(characters arrive back-to-back) we receive one interrupt every,
approximately, 17ms. Even with the line at capacity, we will still
detect "silent" intervals of up to 17ms (or 16 char-times). Since
modbus RTU encoding uses silent intervals to detect frame boundaries,
17ms is the lowest delay we have to use between frame transmissions in
order to be able to reliably determine frames at 9600bps---without
modifying the serial driver and reducing the FIFO fill level for
triggering interrupts. Actually, if we want to do the frame detection
at user-space, we may have to increase this delay *much* more to
account for the O/S multitasking latency. The ModBus spec suggests
delays of approx 3.5ms for this baudrate: Totally untenable.

Using silent intervals to delineate frames is the most brain-dead
decision in an otherwise quite reasonable spec (as these things
go). An HDLC-like framing (had it been selected) would have cleared
all this mess. Know you know who to blame for the horror that follows.

Regardless, we can still cope without special-purpose device
drivers. Quite well for modbus masters, not so well for slaves.

For masters: While we are supposed to detect frames based on silent
intervals, we do not *have* to. Since modbus is a request-response
protocol, if the master stops transmitting requests, the slaves will
stop transmitting responses, and in a short while the line will go
idle and remain so for as long as the master does not transmit
anything. So, for the master to sync with the bus, all it has to do is
stop transmitting requests and wait for the line to go idle for a
reasonable amount of time. After this, it knows that the first byte
received will be the first byte of a response frame (after of course
it transmits a request itself). Responses are formatted in such a way
that by parsing the first few bytes we can determine their length
without relying on silent intervals. Assuming no errors on the
line. If such an error occurs (we can detect it using the frame's CRC,
and should not be often), the master can re-sync by stopping
transmitting and waiting for the line to go idle for a reasonable
amount of time. How much is reasonable?  More than the 16-20ms in the
example above, or whatever can be reliably determined. It should not
matter much, as long as errors are not frequent. In effect the master
has to:

- Synchronize itself by stop transmitting and wait for the line to
  become idle for a reasonable amount of time.

- Parse response characters at the receiver (as they arrive) in order
  to determine the response frame's length.

- Make sure that there is a minimal delay between the reception of a
  response, and the transmission of the next request (which is easy to
  do). This is to avoid confusing slaves that use silent intervals to
  detect frame boundaries.

For slaves things are much worse: Even if we somehow manage to sync
the slave to the beginning of a frame, it is impossible for it to
determine the length of the frame by parsing the bytes as they
arrive. It is so because the slave has no way of knowing whether a
given frame is a response or a request and cannot decide what format
to expect. Yes, you read correctly: There is no way to tell apart
requests from responses, not by themselves. The only possible solution
is this: We somehow manage to sync the slave to the beginning of a
frame that we know is a request frame. Then, assuming that no unknown
frames are exchanged between the master and any of the slaves, and no
errors occur, our slave can keep itself synchronized by receiving and
parsing all the traffic on the bus.


## Timing Parameters

The following timing parameters are defined for operation as master:

- The master timeout MT: is the time the master will wait for the
  first byte of a slave response (counting from the transmission of
  the last request byte), before the master re-transmits the
  request.

- The master frame-timeout MT(fr): Is the time the master will wait
  between bytes of a response before considering the response partial
  and re-transmitting the request.

- The master sync delay MSD: Is the length of silence on the bus the
  master will wait-for in order to consider itself re-synchronized
  (ready to transmit the next request). The master re-synchronizes
  itself when it receives a bad frame (frame whose size it cannot
  determine or frame with a CRC error).

- The master max sync wait (MSW): The amount of time the master will
  wait to become synchronized, before declaring an error and
  abandoning the effort. This timeout could very well be infinite.

And these for operation as slave:

- The slave timeout ST: is the time the slave will wait for the response
  (of anther slave) from the time the last request byte was
  received. After that time, the slave will consider that the request
  will not be replied, and switch to waiting for a new request.

- The slave frame timeout ST(fr): is the time the slave will wait
  between bytes of a response before it considers the response partial
  and switches to listening for a request. Also the time the slave
  will wait between bytes of a request before it considers the request
  partial and switches to waiting for a new request.

- The slave sync delay SSD: Is the length of silence on the bus the
  slave will wait-for in order to consider itself
  re-synchronized. Synchronized, means reasonably certain that the
  next byte on the bus will be the start of a request. The slave
  re-synchronizes itself when it receives a bad frame (frame whose
  size it cannot determine or frame with a CRC error).

- The slave max sync wait (SSW): The amount of time the slave will
  wait to become synchronized, before declaring an error and
  abandoning the effort. This timeout could very well be infinite.


## Setting the timing parameters

Taking the following as given:

1. Once a slave receives a request it will start replying within a
time T(sr), or not at all.

2. Once a slave has started transmitting a response, if it does not
transmit a char for T(fr), then it will not transmit any more, until
it receives a new request. Stated differently: Bytes in a
slave-response are not transmitted more than T(fr) apart, and a slave
transmits only a single response to a master request.

3. Once a master has started transmitting a request, and while the
request has not been fully transmitted, if it does not transmit a byte
for T(fr), then the next byte on the line will also be from the
master, and it will start a new request. Stated differently: Bytes in
a master request are not transmitted more than T(fr) apart, and slaves
do not reply to partial requests.

Given these, you can configure the timeouts, on the master and the
slave to satisfy the following:

For the master:

- Master timeout (MT):  T(sr) < MT,
- Master frame timeout MT(fr): T(fr) < MT(fr)
- Master sync delay MSD: T(fr) < MSD
- Master max sync wait MSW: MSD <<< MSW (possibly infinite)
 
For the slave:

- Slave timeout (ST): T(sr) < ST < MT
- Slave frame timeout ST(fr): T(fr) < ST(fr) < MT(fr)
- Slave sync delay SSD: T(sr) < SSD < MT, [and T(fr) < SSD]
- Slave max sync wait SSW: SSD <<< SSW (possibly infinite)

Notes:

- You should be *very* conservative with the Frame Timeouts (master
  and slave) to account for latency introduced by hardware queueing
  and buffering by the driver, and the O/S. Being conservative will
  *not* degrade performance. These timeouts only protect against
  partial replies (e.g. slave powered-off mid-frame) which are very
  rare.

- For the slave (implemented without using silent intervals to detect
  frame boundaries, as described in this document) the tricky part is
  the correct configuration of ST and SSD. They MUST be larger than
  T(sr) and T(fr) AND smaller than MT. If this not satisfied, the
  slave will not be able to synchronize.


## Example

We know that our slaves will always start transmitting a reply, no
latter than 50ms after they receive the request. We also know that our
slaves transmit all response bytes back-to-back, and the same is true
for the master requests.

For the master:

- Timeout, master [MT]: We add 20ms x 2 to the 50ms to account for
  20ms queuing and buffering latency (hardware, driver, or OS induced)
  at the slave side and another 20ms at the master. This brings us to
  90ms. Being extra conservative we start with a value near 180ms.

- Frame Timeout, master [MT(fr)]: We assume 20ms latency, as above,
  and being extra conservative we start with 90ms (doesn't affect
  performance, anyway).

- Sync Delay, master [MSD]: Same as MT(fr) 

For the slave:

- Timeout, slave [ST]: Between 90ms and whatever you set MT. We start
  with 130ms.

- Frame Timeout, slave [ST(fr)]: Less than MT(fr) but more than
  20ms. Start with 60ms.

- Sync Delay, slave [SSD]: Same as ST


## Final Remarks

Longer timeouts can never cause malfunction. An overly large Master /
Slave Timeout will cause delays if a slave is powered-down or
disconnected. An overly large Master / Slave Frame Timeout will only
cause delays when a slave is powered-down mid-frame (which is very
rare).

Remember that the timeouts you set must account for the latency in
your system (including O/S and multitasking delays). Depending on your
system, do not count to be able to be accurate for anything bellow
10ms-20ms. For example when we say that ST < MT, the difference must
be at least 10ms-20ms.

Timeouts have no effect, performance or otherwise, when the system is
functioning correctly (no errors on the line, no missing slaves).


[[[ ## Random thoughts ## ]]]

Assume the unusual case where T(sr) < T(fr), and we configure the
master with:

  T(sr) < MT < T(fr) < MT(fr)

For the slave we then have two options. Both bad:

  1. Set T(fr) < SSD < MT(fr). In this case we reliably synchronize
     only when we have a partial frame condition, which might never
     happen

  2. Set T(sr) < SSD < MT < T(fr). Then we may sync to the gap between
     bytes in a slave's reply (false sync).

The correct solution would be to set:

  Master: T(sr) < T(fr) < MT / MT(fr)

and

  Slave: T(sr) < T(fr) < SSD < MT

